---
title: "Do not judge a book by its cover... but a person maybe!"
output:
  pdf_document: default
  html_document:
    df_print: paged
header-includes:
  - \usepackage{titling}
  - \setlength{\droptitle}{-2em} 
subtitle: 'Evidence from a speed dating dataset reveals how to bag yourself a match'
---
<!-- ![](speed-dating-image) -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      eval = T,
                      message = F)

# We add the libraries we will need
library(knitr)
library(arm)
library(foreign)
library(magrittr)
library(dplyr)
library(tidyverse)
library(car)
library(xtable)
library(ggplot2)
library(glmnet)
library(adapt4pv)
library(gridExtra)
library(GGally)
library(ggpubr)
library(leaps)
library(bestglm)
library(boot)
library(tree)
library(kableExtra)
library(rpart.plot)
# We add this to remove xtable comments
options(xtable.comment = FALSE)
#To have centred titles
theme_update(plot.title = element_text(hjust = 0.5))
# ~
```
\vspace{-6truemm}

Everyone wants to find _the one_. However, approaching an attractive stranger at the bar seems so frightening (and somehow old-fashioned). Nowadays, indeed, the quest for love is pursued online: Tinder, Hinge, and other dating apps have become a sort of norm to meet potential partners. Before the arrival of smartphones, a good compromise was _Speed Dating_: started as a programme for Jewish singles in Los Angeles, it soon spread all over the world as a new way for single people to meet each other. The format is simple: a group of singles gathers at a cafe or similar venue. Armed with a scorecard and their sparkling personality, they are paired up and their first date begins. After four minutes of conversation, a bell rings and the men proceed to the next lady. Participants mark on the scorecard whether they would have an interest in meeting their date again. If there is mutual interest, the organizers will provide the couple with the contact information. From that point on, everything is up to you. The main goal is therefore to get a match. The question is: what makes you click with a person in a few minutes? We would like to have a better insight on how individual choices among random participants are made. To give an answer, we will explore the [data](http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/?C=N;O=D) behind the paper ["Gender differences in mate selection: evidence from a speed dating experiment"](http://www.stat.columbia.edu/~gelman/stuff_for_blog/sheena.pdf). This data collects the surveys filled out by the participants in Speed Dating events held at Columbia University from 2002 to 2004. Our goal is to select the main qualities we look for when selecting a partner.
```{r data, include=FALSE}
# We import the data from the following website:
# http://www.stat.columbia.edu/~gelman/arm/examples/speed.dating/?C=N;O=D
# A pdf explaining the variables can be found at: https://perso.telecom-paristech.fr/eagan/class/igr204/data/SpeedDatingKey.pdf 

data = read.csv("Speed Dating Data.csv")

# We remove from the dataset the waves from 6 to 9 and the respective variables
# (the evaluations are on a 1-10 scale instead of 1-100)
# We also remove the variables which we are not interested in

new_data = data %>% 
  filter(!(wave %in% 6:9)) %>%
  subset(select = - c((76:81),(86:97),(107:120),(145:193))) %>%
  # the data 76:81 was collected only after the 6th wave and is about what
  # you think MOST of your fellow men/women look for in the opposite sex:
  # we are more interested in subjective opinions rather than generalizations
  # non relevant variables of TIME 1: 86:97
  # non relevant variables collected during the event: 107:120
  # non relevant variables of TIME 2: 145:154
  # all variables of TIME 3: 155:193
  subset(select = - c(idg, condtn, field, career, undergra, round, order, wave,
                      tuition, zipcode, income, expnum, position, positin1, id, partner)) %>%
  # the decision of the subject, match, dec_o, race, samerace and the gender must be factors
  mutate(dec_o = factor(dec_o, levels=c(0,1), labels=c('No','Yes')),
         gender = factor(gender, levels=c(0,1), labels=c('Female','Male')),
         samerace = factor(samerace, levels=c(0,1), labels=c('No','Yes')),
         race = factor(race, levels=1:6, labels=c("Black", "Caucasian", "Hispanic",
                                                  "Asian", "Native American", "Other")),
         iid = as.factor(iid), pid = as.character(pid),
         field_cd = factor(field_cd, levels=1:18,
                           labels=c("Law", "Math", "Psychologist", "Medical Science",
                                    "Engineering", "English","History", "Business",
                                    "Education", "Biological Sciences", "Social Work",
                                    "Undergrad", "Political Science", "Film",
                                    "Fine Arts", "Languages", "Architecture", "Others")),
         studies = fct_collapse(field_cd, # we consider a coarser partition of the field of studies
                                Science = c("Math","Engineering", "Biological Sciences"),
                                `Medical Science` = c("Psychologist", "Medical Science"),
                                Business = c("Business"),
                                Humanities = c("Law", "English", "History", "Education",
                                               "Social Work", "Political Science",
                                               "Film", "Fine Arts", "Languages"),
                                Others = c("Undergrad", "Architecture", "Others")))
```
```{r, include=FALSE}
# We adjust the dataset: there are some evaluations higher than 10,
# but we are using a 0-10 scale
new_data[which(new_data$attr_o > 10),"attr_o"] = 10
# Moreover we notice that when the age of the subject is missing, the observations
# is pretty much useless, as it has only the data on the partner: we remove them
new_data = new_data %>%
  subset(!is.na(age)) 
```
```{r data_desc, include=FALSE}
# We create the dataset we will use for exploration by considering the attributes
# referring to each of the participants
data_desc = new_data %>%
  select(c(iid, gender, age, race, field_cd, studies, career_c)) %>%
  merge(aggregate(. ~ iid, data=select(new_data, c(iid, imprace, imprelig, attr1_1:shar1_1)), FUN=mean),
        by.x="iid", by.y="iid") %>%
  distinct(iid, .keep_all=T)
rownames(data_desc) = data_desc$iid
data_desc = select(data_desc, -iid)
```
```{r, include=FALSE}
# We add some features that could be interesting for our analysis:
# age_gap: is the age difference between the male participant and the female one;
# it is positive if the male is older, negative otherwise
# same_studies: this variable says whether the two participants are attending courses
# in the same field
new_data = new_data %>%
  mutate(age_gap = case_when(gender == 'Female' ~ age_o - age,
                             gender == 'Male' ~ age - age_o),
         same_studies = case_when(data_desc[pid,'studies'] == studies ~ 1,
                                  TRUE ~ 0) %>%
           factor(levels=c(0,1), labels=c("No","Yes")))
```

## Exploratory analysis of the data
```{r, include=FALSE, warning=FALSE}
female_number = round(length(which(data_desc$gender=='Female'))/dim(data_desc)[1]*100, 2) 
gender_age_gap = round((mean(subset(data_desc, gender=='Male')$age) -
                          mean(subset(data_desc, gender=='Female')$age)),2)
```
The dataset is reasonably even on the genders: `r toString(female_number)`% of the participants is indeed female. Gender is also balanced among races: most of the participants are Caucasian, but the proportion of Asians is also relevant. There is however a difference on the fields of study: while there are more females studying Humanities or Medical Sciences, most males are attending Business or Science courses. Most of the participants are in their twenties and males are in mean older of a few months.
```{r gender differences, echo=FALSE, warning=FALSE, fig.height=2, fig.width=8, fig.align='center'}
race_plot = ggplot(data_desc,
                   aes(x = race, fill = gender)) +
  geom_bar(color = 'black') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Counts") + xlab(NULL) + ggtitle("Race") +
  scale_fill_manual(values=c("pink", "royalblue"))

studies_plot = ggplot(subset(data_desc, !is.na(studies)),
                     aes(x = studies, fill = gender)) +
  geom_bar(color = 'black') +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Counts") + xlab(NULL) + ggtitle("Field of study") +
  scale_fill_manual(values=c("pink", "royalblue"))

age_plot = ggplot(data_desc) +
  geom_histogram(aes(x = age, fill = gender), col = 'black',
                  breaks = seq(min(data_desc$age)-4, max(data_desc$age)+5, 2)) +
  ylab("Counts") + xlab(NULL) + ggtitle("Age") +
  scale_fill_manual(values=c("pink", "royalblue"))

age_box = ggplot(data_desc, aes(x = age, fill = gender, y=gender)) +
  geom_boxplot() +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed") +
  geom_text(data = aggregate(age~gender, data_desc, mean),
            aes(x = age, y = gender, label=round(age,2), vjust=4, col = gender),
            size = 2.5, fontface = 'bold') +
  ylab("Gender") + xlab(NULL) + ggtitle(" ")  +
  scale_colour_manual (values=c("pink", "royalblue")) +
  scale_fill_manual(values=c("pink", "royalblue")) +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())
 
ggarrange(race_plot, studies_plot, age_plot, age_box, common.legend = TRUE, ncol=4, legend = "right") %>%
  annotate_figure(top = text_grob("Differences between genders", face = "bold", size = 13))
```

```{r, include=FALSE}
# We create one dataset per gender to see if preferences are different 
data_female = new_data %>% 
  filter(gender == 'Female')
data_male = new_data %>% 
  filter(gender == 'Male')

temp_data = subset(data_female, !is.na(age_gap))
percent_older_male = round(length(which(temp_data$age_gap>0))/dim(temp_data)[1]*100, 2)
```
On the other hand, it could be more of interest to consider the age gap between the male and the female of each couple, as it could have an influence over the decision. Ages are quite balanced between the genders: the graph shows that in `r toString(percent_older_male)`% of the dates the male is older. However, the decision variable follows the shape of age gap both for both gender, hinting at the fact that age difference might not be really taken into consideration when selecting a partner. At the end of the day, age is really just a number.
```{r age gap, echo=FALSE,  warning=FALSE, fig.height=1.35, fig.width=6, fig.align='center'}
temp_data = subset(data_female, !is.na(age_gap))
agegap_female = ggplot(temp_data,
                       aes(x = age_gap, fill = factor(dec,
                                                      levels=c(0,1),
                                                      labels=c("No","Yes")))) +
  geom_histogram(breaks = seq(min(temp_data$age_gap)-2, max(temp_data$age_gap)+2, 2),
                 color = 'black') +
  ggtitle("Female") +
  ylab("Counts") + xlab(NULL) + labs(fill = 'Decision') +
  theme(plot.title = element_text(size=11))  +
  scale_fill_manual(values=c("red2", "springgreen3"))

temp_data = subset(data_male, !is.na(age_gap))
agegap_male = ggplot(temp_data, aes(x = age_gap, fill = factor(dec,
                                                      levels=c(0,1),
                                                      labels=c("No","Yes")))) +
  geom_histogram(breaks = seq(min(temp_data$age_gap)-2, max(temp_data$age_gap)+2, 2),
                 color = 'black') +
  ggtitle("Male")+
  ylab("Counts") + xlab(NULL) + labs(fill = 'Decision') +
  theme(plot.title = element_text(size=11), axis.title.y = element_blank())  +
  scale_fill_manual(values=c("red2", "springgreen3"))

ggarrange(agegap_female, agegap_male, common.legend = TRUE, ncol=2, legend = "right") %>%
  annotate_figure(top = text_grob("Preferences on the age gap", face = "bold", size = 11.5))
```
```{r, include=FALSE}
field_plot = ggplot(subset(data_desc, !is.na(field_cd)), aes(x=field_cd, fill=gender)) +
  geom_bar(color = 'black') +
  theme(axis.text.x = element_text(angle=70, hjust=1))
```
Also issues such as race and religion play much less of a role than expected.
When registering to the Speed Dating events, participants were asked to give also some information on the importance of having the same background as the partner.  We can see from the histograms that overall they do not attach great importance to having the same race and professing the same religion. However, it seems to be slightly more relevant to females than males and to Caucasian and Asian people.
```{r same background, echo=FALSE,  warning=FALSE, fig.height=2, fig.width=8, fig.align='center'}
# We want to see if having the same race or the same religion is important when selecting a potential partner.

# Importance same background according to the race
  # same race
race_plot = ggplot(data = subset(data_desc, race!='Other'), aes(x = imprace, fill = race)) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Counts") + xlab(NULL) + geom_bar() +
  ggtitle("Same race")  +
  scale_fill_manual(values=c("red2", "#F0E442", "#009E73", "#6699CC"))

race_bp = ggplot(data = subset(data_desc, race!='Other'), 
                 aes(x=race, y=imprace, fill=race)) +
  geom_boxplot() +
  labs(x = "Race", y = "Importance same race", fill = 'Race') +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed") +
  theme(axis.title=element_blank(),
        axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  ggtitle(" ") +
  scale_fill_manual(values=c("red2", "#F0E442", "#009E73", "#6699CC"))

  # same religion
religion_plot = ggplot(data = subset(data_desc, race!='Other'), aes(x = imprelig, fill = race)) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Counts") + xlab(NULL) + geom_bar() +
  ggtitle("Same religion")  +
  scale_fill_manual(values=c("red2", "#F0E442", "#009E73", "#6699CC"))

relig_bp = ggplot(data = subset(data_desc, race!='Other'), 
                  aes(x=race, y=imprelig, fill=race)) +
  geom_boxplot() +
  labs(x = "Race", y = "Same religion", fill = 'Religion') +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed") +
  theme(axis.title=element_blank(),
        axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  ggtitle(" ")  +
  scale_fill_manual(values=c("red2", "#F0E442", "#009E73", "#6699CC"))

ggarrange(race_plot, race_bp, religion_plot, relig_bp, common.legend = TRUE, ncol=4, legend = "right") %>%
  annotate_figure(top = text_grob("Same background importance according to race",
                                  face = "bold", size = 13))


# Importance same backgroud according to the gender
  # same race
race_plot = ggplot(data = data_desc, aes(x = imprace, fill = gender))+
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Counts") + xlab(NULL) + geom_bar(color = 'black') + ggtitle("Same race") +
  scale_fill_manual(values=c("pink", "royalblue"))

race_gender_bp = ggplot(data = data_desc, 
                 aes(x=gender, y=imprace, fill=gender)) +
  geom_boxplot() +
  labs(x = "Gender", y = "Importance same race", fill = 'Gender') +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed") +
  theme(axis.title=element_blank(),
        axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  ggtitle(" ") +
  scale_fill_manual(values=c("pink", "royalblue"))

  # same religion
religion_plot = ggplot(data = data_desc, aes(x = imprelig, fill = gender)) +
  theme(axis.text.x = element_text(angle=45, hjust=1)) +
  ylab("Counts") + xlab(NULL) + geom_bar(color = 'black') + ggtitle("Same religion") +
  scale_fill_manual(values=c("pink", "royalblue"))

relig_gender_bp = ggplot(data = data_desc, 
                  aes(x=gender, y=imprelig, fill=gender)) +
  geom_boxplot() +
  labs(x = "Gender", y = "Importance same religion", fill = 'Gender') +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed") +
  theme(axis.title=element_blank(),
        axis.text.x=element_blank(), axis.ticks.x=element_blank()) +
  ggtitle(" ") +
  scale_fill_manual(values=c("pink", "royalblue"))

ggarrange(race_plot, race_gender_bp, religion_plot, relig_gender_bp, common.legend = TRUE, ncol=4, legend = "right") %>%
  annotate_figure(top = text_grob("Same background importance according to gender",
                                  face = "bold", size = 13))
```
```{r same background frequencies, include=FALSE, eval=F, fig.height=4, warning=FALSE}
# Alternative plots to understand the shape of data,
# we believe that the previous ones provide a better visualization

imprace_p = ggplot(data =  subset(data_desc, race!='Other'), aes(x = imprace)) +
  geom_histogram(fill='light blue', breaks=seq(0,11,1), color='black')
imprelig_p = ggplot(data = data_desc, aes(x = imprelig)) +
  geom_histogram(fill='light blue', breaks=seq(0,11,1), color='black')
ggarrange(imprace_p,imprelig_p, ncol=2, common.legend=T)

race_plot = ggplot(data =  subset(data_desc, race!='Other'), aes(x = imprace, fill = race, y = 1)) +
  labs(y="Counts", x = "Importance same race") + 
  geom_col(position = "fill")
religion_plot = ggplot(data = data_desc, aes(x = imprelig, fill = race, y = 1)) +
  labs(y="Counts", x = "Importance same religion") +
  geom_col(position = "fill")
ggarrange(race_plot, religion_plot, common.legend = TRUE, ncol=2)

race_plot2 = ggplot(data = data_desc, aes(x = imprace, fill = gender, y = 1)) +
  labs(y="Counts", x = "Importance same race") + 
  geom_col(position = "fill") 
religion_plot2 = ggplot(data = data_desc, aes(x = imprelig, fill = gender, y = 1)) +
  labs(y="Counts", x = "Importance same religion") + 
  geom_col(position = "fill")
ggarrange(race_plot2, religion_plot2,  common.legend = TRUE, ncol=2)
```
```{r perc_yes, include=FALSE}
# How many times in mean does a female say Yes among all the dates?
perc_female_yes = data_female %>%
  select(c(iid, dec)) %>%
  group_by(iid) %>%
  summarise(dec_p = mean(dec))
perc_female_yes = (100 * mean(perc_female_yes$dec_p)) %>% round(2)
# How many times in mean does a male say Yes among all the dates?
perc_male_yes = data_male %>%
  select(c(iid, dec)) %>%
  group_by(iid) %>%
  summarise(dec_p = mean(dec))
perc_male_yes = (100 * mean(perc_male_yes$dec_p)) %>% round(2)
```
Participants had also to rate the importance of six attributes in a potential partner by distributing 100 points among them.
The boxplots of the ratings on _attractive_ immediately catch the eye: it is clear indeed that males seem to put great emphasis on the physical appearance. However, males' ratings of the other attributes seem to be quite even, with the exception of _ambitious_ which does not seem quite relevant. Females' ratings are instead more balanced, suggesting that all qualities matter for them: how picky!
Last but not least, men decided to stay in touch with the partner `r toString(perc_male_yes)`% of the times, while women `r toString(perc_female_yes)`%. These percentages are not low, suggesting that speed dating could really help to find the other half of the apple.
```{r boxplot rating attributes, echo=FALSE, fig.heigth = 2.5, fig.width=11, fig.align='center'}
# We plot the boxplots of the 6 attributes, dividing them by gender and setting the y limit to (0,100) for easier comparison

attr_bp = ggplot(data = data_desc, aes(y = attr1_1, x = gender, fill=gender)) +
    geom_boxplot() + ggtitle("Attractive") +
  theme(legend.position = "none", plot.title = element_text(size=10),
        axis.title = element_blank(), axis.text.x = element_blank(),
        axis.ticks.x = element_blank()) +
  ylim(0,100) +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed")+
  scale_fill_manual(values=c("pink", "royalblue"))

sinc_bp = ggplot(data = data_desc, aes(y = sinc1_1, x = gender, fill=gender)) +
  geom_boxplot() + ggtitle("Sincere") +
  theme(axis.title = element_blank(), axis.text.x = element_blank(),
        axis.ticks.x = element_blank(), plot.title = element_text(size=10)) +
  ylim(0,100) +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed")+
  scale_fill_manual(values=c("pink", "royalblue"))

intel_bp = ggplot(data = data_desc, aes(y = intel1_1, x = gender, fill=gender)) +
  geom_boxplot() + ggtitle("Intelligent") +
  theme(axis.title = element_blank(), axis.text.x = element_blank(),
        axis.ticks.x = element_blank(), plot.title = element_text(size=10)) +
  ylim(0,100) +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed")+
  scale_fill_manual(values=c("pink", "royalblue"))

fun_bp = ggplot(data = data_desc, aes(y = fun1_1, x = gender, fill=gender)) +
  geom_boxplot() + ggtitle("Fun") +
  theme(axis.title = element_blank(), axis.text.x = element_blank(),
        axis.ticks.x = element_blank(), plot.title = element_text(size=10)) +
  ylim(0,100) +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed")+
  scale_fill_manual(values=c("pink", "royalblue"))

amb_bp = ggplot(data = data_desc, aes(y = amb1_1, x = gender, fill=gender)) +
  geom_boxplot() + ggtitle("Ambitious") +
  theme(axis.title = element_blank(), axis.text.x = element_blank(),
        axis.ticks.x = element_blank(), plot.title = element_text(size=10)) +
  ylim(0,100) +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed")+
  scale_fill_manual(values=c("pink", "royalblue"))

shar_bp = ggplot(data = data_desc, aes(y = shar1_1, x = gender, fill=gender)) +
  geom_boxplot() + ggtitle("Interests") +
  theme(axis.title = element_blank(), axis.text.x = element_blank(),
        axis.ticks.x = element_blank(), plot.title = element_text(size=10)) +
  ylim(0,100) +
  stat_summary(geom = "errorbar", fun.min = mean, fun = mean, fun.max = mean,
               width =.75, linetype = "dashed")+
  scale_fill_manual(values=c("pink", "royalblue"))

ggarrange(attr_bp, sinc_bp, intel_bp, fun_bp, amb_bp, shar_bp,
          ncol=6, nrow=1, common.legend=T, legend='right') %>%
  annotate_figure(top = text_grob("Ratings of the attributes according to gender",
                                  face = "bold", size = 15))
```

##  It is time to drop the big gun: logistic regression
#### Logistic regression
Now we would like to support our deductions with some Statistics. In particular, we would like to regress the variable _decision_ with a generalized linear model: the logistic regression. By doing so, we will measure the relationship between the scores that one assigned to the partner and the decision made.
As suggested from the data exploration, the six attributes could have an influence on the choice of a potential partner.
If we have a look at the correlation between decision and each one of these variables, we see that some of them could be more relevant: we will investigate this later with the regression. 
```{r regression dataset with 6 attributes, include=FALSE}
# We create a smaller dataset with the variables of interest for the regression
data_reg_male6 = select(data_male,c(dec:shar)) %>% na.omit()

data_reg_female6 = select(data_female,dec:shar) %>% na.omit()

npred = ncol(data_reg_male6) - 1
```
```{r correlation decision vs 6 attributes, echo=FALSE, fig.align='center'}
correlation = as.data.frame(rbind(round(cor(data_reg_male6), 3)[1,2:7]))
#correlation = transpose(correlation)
colnames(correlation)=c('Attractive', 'Sincere', 'Intelligent', 'Fun', 'Ambitious', 'Interests')
rownames(correlation) = 'Decision'

kable(correlation, align='c', booktabs=T) %>%
  kable_styling(font_size = 7.5) %>%
  kable_styling(latex_options = 'HOLD_position')
```
```{r correlation, eval=FALSE, include=FALSE}
# In order to visualize possible patterns of the predictors
# we used the following graphs, changing x and y,
# we didn't show the matter in the paper for space reasons
ggplot(data_reg_male6) +
  geom_count(aes(x=attr, y=sinc, color=..n..)) +
  scale_x_continuous(expand=c(.2,.2)) +
  scale_y_continuous(expand=c(.2,.2)) +
  scale_fill_continuous(guide = "colourbar")

ggplot(data_reg_male6) +
  geom_hex(aes(x=attr, y=fun), binwidth=c(1,1)) +
  scale_x_continuous(expand=c(.2,.2)) +
  scale_y_continuous(expand=c(.2,.2))

ggplot(data_reg_male6) +
  geom_bin_2d(aes(x=attr, y=fun), binwidth=c(1,1)) +
  scale_x_continuous(expand=c(.2,.2)) +
  scale_y_continuous(expand=c(.2,.2))
cor(data_reg_male6$attr,data_reg_male6$fun)
```
```{r PCA, eval=FALSE, include=FALSE}
# We try the PCA, but we do not find it relevant
P = prcomp(select(data_reg_male6,-dec), scale=F)
P$rotation
```
```{r significance function, include=FALSE}
significance = function(pval){
  # input: vector of p-values
  # output: vector that contains an asterisk in the i-th entry if 
  #         the i-th p-value is small enough 
  ast = c()
  for(i in 1:length(pval)){
    if(pval[i]<0.05){
    ast = c(ast,'*')
    }
    else{
      ast = c(ast,' ')
    }
  }
  return(ast)
}
```
```{r BSSplot function, include=FALSE}
bestsubset_plot = function(df){
  # input: the dataframe labeled as Subsets in the list returned by bestglm function
  # ouput: a plot with ggplot2 that mimic the plot given by the output of
  #        regsubsets in the leaps package according to BIC criterion
  sdf = df[order(-df$BIC),]
  p = ncol(sdf)-2
  x = c()
  y = c()
  color = c()
  for(j in 1:p){
    for(i in 1:nrow(sdf)){
      x = c(x,j)
      y = c(y,i)
      if(sdf[i,j]){
        color = c(color, 'black')
      }
      else{
        color = c(color, 'white')
      }
    }
  }
  data_plot = data.frame(x, y, color)
  plot = ggplot(data = data_plot) +
    geom_rect(aes(xmin=x-.5, xmax=x+.5, ymin=y-.5, ymax=y+.5), fill=color, color='grey50') +
    theme(axis.text.x = element_text(angle=90, vjust=.5),
          axis.ticks.x = element_blank(),
          panel.border = element_rect(linetype = "solid", fill = NA)) +
    scale_x_discrete(limits = colnames(sdf)[1:p], expand=c(0,0)) +
    scale_y_continuous(breaks=1:p, labels = round(sdf$BIC,0), expand=c(0,0)) +
    coord_cartesian(xlim=c(.5,p+.5), ylim=c(.5,p+.5))
  return(plot)
}
```
The evaluation of a partner could take also into consideration the age gap, the cultural background and the field of study.
We fit therefore a logistic regression model on the following attributes:
```{r regression dataset, include=FALSE}
# We create a smaller dataset with the variables of interest for the regression
# We decide to omit rows with NA since we have plenty of observations
data_reg_male_all = select(data_male,c(dec:shar, age_gap, same_studies, samerace)) %>%
  na.omit()

data_reg_female_all = select(data_female,c(dec:shar, age_gap, same_studies, samerace)) %>%
  na.omit()
```
```{r glm with all the predictors, echo=FALSE, warning=FALSE, fig.align='center'}
# We fit a linear regression model to the 6 attributes + other 3 to see their influence on the decision
fit_male_all = glm(data = data_reg_male_all, dec ~., family = 'binomial')
male_pv_all = summary(fit_male_all)$coefficients[,4]
male_coef_all = as.character(round(fit_male_all$coefficients,2)) %>%
  paste(significance(male_pv_all), sep=' ') 
# we append to each coefficient an asterisk whenever the corresponding p-value is less than 0.05

fit_female_all = glm(data = data_reg_female_all,
                 dec ~., family = 'binomial')
female_pv_all = summary(fit_female_all)$coefficients[,4]
female_coef_all = as.character(round(fit_female_all$coefficients,2)) %>%
  paste(significance(female_pv_all), sep=' ')

glm_fit_all = as.data.frame(rbind(Female = female_coef_all, Male = male_coef_all))
colnames(glm_fit_all) = c('Intercept', 'Attractive', 'Sincere', 
                          'Intelligent', 'Fun', 'Ambitious', 
                          'Interests', "Age gap", "Studies", "Race")
 kable(glm_fit_all, digits=3, label = 'Gender differences in attributes weights', align='c', booktabs=T) %>%
  kable_styling(font_size = 7.5) %>%
  kable_styling(latex_options = 'HOLD_position')

```
At a first glance, we can see that physical appearance is crucial when selecting a possible partner, especially for males. Being fun and sharing same interests also seem to be relevant for both genders, whereas attending lectures in the same field of study appears to be taken into consideration only by males.
We apply best subset selection (BSS) to understand which variables really matter for the regression.
```{r BSS, echo=FALSE, fig.width=9, fig.height=3, cache=T}
npred = ncol(data_reg_female_all)-1

# BSS for males
design_male = data_reg_male_all[,c(2:(npred+1),1)] # we create the design matrix
bestsub_male = bestglm(design_male, family = binomial) # we perform best subset selection
colnames(bestsub_male$Subsets)[1:(npred+1)] = c("Intercept", "Attractive", "Sincere", 
                                   "Intelligent", "Fun", "Ambitious", "Interests",
                                   "Age gap", "Studies", "Race")
# we plot the result
male_bestp = bestsubset_plot(bestsub_male$Subsets) +
  ggtitle("Male")

# BIC visualization
BIC_malep = ggplot(data=bestsub_male$Subsets[-1,-1], aes(x=1:npred, y=BIC)) + #exclude intercept
  geom_line() +
  scale_x_continuous(breaks=1:npred) +
  theme(panel.grid.minor.x = element_blank()) +
  geom_vline(xintercept=5, col='orange', linetype='dashed') +
  xlab('Number of predictors')

# BSS for females
design_female = data_reg_female_all[,c(2:(npred+1),1)]
bestsub_female = bestglm(design_female, family = binomial)
colnames(bestsub_female$Subsets)[1:(npred+1)] = c("Intercept", "Attractive", "Sincere", 
                                   "Intelligent", "Fun", "Ambitious", "Interests",
                                   "Age gap", "Studies", "Race")
female_bestp = bestsubset_plot(bestsub_female$Subsets) +
  ggtitle("Female")

BIC_femalep = ggplot(data=bestsub_female$Subsets[-1,-1], aes(x=1:npred, y=BIC)) +
  geom_line() +
  scale_x_continuous(breaks=1:npred) +
  theme(panel.grid.minor.x = element_blank()) +
  geom_vline(xintercept=4, col='orange', linetype='dashed') +
  xlab('Number of predictors')

ggarrange(female_bestp, BIC_femalep, male_bestp, BIC_malep, ncol=4) %>%
  annotate_figure(top = text_grob("Best subsect selection",
                                  face = "bold", size = 13))
```
BIC criterion for BSS suggests the use of two different sets of predictors according to the genders:

* Attractive, Fun, Ambitious and Shared interests for __Females__
* Attractive, Sincere, Fun, Ambitious and Shared interests for __Males__

Moreover the best subsets are nested, inducing a natural order of relevance in the prediction of the decision:

* Attractive, Interests, Fun, Ambitious, Intelligent, Sincere, Studies, Race and Age gap for __Females__
* Attractive, Interests, Ambitious, Fun, Sincere, Studies, Intelligent, Race and Age gap for __Males__
```{r anova, include=F}
glm_male4 = glm(dec ~ attr + fun + amb + shar,
                data=data_reg_male_all, family='binomial')
glm_male5 = glm(dec ~ attr + sinc + fun + amb + shar,
                data=data_reg_male_all, family='binomial')
glm_male6 = glm(dec ~ attr + sinc + fun + amb + shar + same_studies,
                data=data_reg_male_all, family='binomial')

anova(glm_male4, glm_male5, glm_male6, test='Chi')
# p-value is not that low for the third model, and deviance does not change that much,
# we prefer to stick with the model chosen according to BIC criterion

glm_female3 = glm(dec ~ attr + fun + shar,
                  data=data_reg_female_all, family='binomial')
glm_female4 = glm(dec ~ attr + fun + amb + shar,
                  data=data_reg_female_all, family='binomial')
glm_female5 = glm(dec ~ attr + intel + fun + amb + shar,
                  data=data_reg_female_all, family='binomial')
glm_female6 = glm(dec ~ attr + intel + sinc + fun + amb + shar,
                  data=data_reg_female_all, family='binomial')
anova(glm_female3, glm_female4, glm_female5, glm_female6, test='Chi')

# In order to get a significant decrease of deviance, 
# we would need to add 2 predictors to the model,
# this is also hinted by the shape of the BIC curve.
# Again we decide to stick with the model selected before,
# as adding one predictor (intelligence) would not be that significant
# and two new predictors are too much
```
Performing further testing with anova, we reckon that being sincere is actually relevant only for males and adding other predictors to both models would not be that significant.
We then fit a logistic regression with the selected predictors:
```{r chosen glm, echo=FALSE, warning=FALSE, fig.align='center'}
# We fit the linear regession on the chosen predictors to see their influence on the decision
# We extract the dataset again, in order to discard less observations
data_reg_male_bss = select(data_male,c(dec, attr, sinc, fun, amb, shar)) %>% na.omit
fit_male_bss = glm(data = data_reg_male_bss,
               dec ~., family = 'binomial')
male_pv_bss = summary(fit_male_bss)$coefficients[,4]
male_coef_bss = as.character(round(fit_male_bss$coefficients,2)) %>%
  paste(significance(male_pv_bss), sep=' ')
# we append to each coefficient an asterisk whenever the corresponding p-value is less than 0.05

data_reg_female_bss = select(data_female,c(dec, attr, fun, amb, shar)) %>% na.omit
fit_female_bss = glm(data = data_reg_female_bss,
                 dec ~., family = 'binomial')
female_pv_bss = summary(fit_female_bss)$coefficients[,4]
female_coef_bss = as.character(round(fit_female_bss$coefficients,2)) %>%
  paste(significance(female_pv_bss), sep=' ')
female_coef_bss = c(female_coef_bss[1:2],'--',female_coef_bss[3:5])
# we add an empty slot to make the two coefficients vectors coherent

glm_fit_bss = as.data.frame(rbind(Female = female_coef_bss, Male = male_coef_bss))
colnames(glm_fit_bss) = c('Intercept', 'Attractive', 'Sincere', 'Fun', 'Ambitious', 'Interests')
kable(glm_fit_bss, digits=3, label = 'Gender differences in attributes weights', align='c', booktabs=T) %>%
  kable_styling(font_size = 7.5) %>%
  kable_styling(latex_options = 'HOLD_position')
```
We can therefore deduce that the final decision for both genders relies mostly on the physical appearance of the partner, in particular for males. It is indeed difficult to get a grasp on the personality of a complete stranger in four minutes. However, sharing common interests improves the chances of being chosen, as well as being fun.
Yet, both genders think bitterly of ambition and males do not seem to appreciate sincerity: if you want to impress a stranger, it would then be better not to be blunt and reveal too much!

#### Lasso regression
We now want to apply a penalty in order to shrink the coefficient estimates towards zero.
We will consider Lasso technique as we want to get a sparse solution to simplify our model.
```{r male lasso, include=FALSE}
colours = c("red2", "darkorange", "#F0E442", "green", "#009E73", "#6699CC", "#332288", "#AA4499", "pink")
X_m = model.matrix(fit_male_all)[,-1]
Y_m = data_reg_male_all$dec
male_fit_lasso = glmnet(X_m,Y_m, alpha=1, family='binomial')

# BIC criterion selection
n = nobs(male_fit_lasso)
male_lasso_BIC = deviance(male_fit_lasso) + log(n) * (male_fit_lasso$df + 1) # vector of BIC
male_coef_lasso = coef(male_fit_lasso)[,which.min(male_lasso_BIC)] # coefficients chosen with BIC criterion
male_lasso_lambda_BIC = male_fit_lasso$lambda[which.min(male_lasso_BIC)]

# we create a dataframe for the visualization of shrinkage
male_lasso_df = data.frame(norm = abs(coef(male_fit_lasso)[-1,]) %>% apply(2,sum), #L1 norm of coefficients
                           lambda = male_fit_lasso$lambda,
                           coefficient = c(t(as.matrix(coef(male_fit_lasso)))[,-1]),
                           predictor = rep(rownames(coef(male_fit_lasso))[-1],
                                           each=ncol(coef(male_fit_lasso)))) 
                            # tell the corresponding predictor of the coefficient

male_lasso_p = ggplot(male_lasso_df, aes(x=norm, y=coefficient,
                                         col=factor(predictor, labels=c('Age gap','Ambitious',
                                                                        'Attractive', 'Fun',
                                                                        'Intelligent',
                                                                        'Studies', 'Race',
                                                                        'Interests',
                                                                        'Sincere')))) +
  geom_line(size = 0.8) + ggtitle("Male lasso") +
  geom_text(x=sum(abs(male_coef_lasso[-1])), y=0.1, label='BIC selection', colour='black', size=3) +
  geom_vline(xintercept = sum(abs(male_coef_lasso[-1])), linetype='dashed') +
  labs(x = "L1 norm", y = "Coefficients", col = "Predictor") +
  scale_color_manual(values = colours) +
  theme(panel.background = element_rect(fill = "gray95"))
```
```{r female lasso, include=FALSE}
X_f = model.matrix(fit_female_all)[,-1]
Y_f = data_reg_female_all$dec
female_fit_lasso = glmnet(X_f,Y_f, alpha=1, family='binomial')

# BIC criterion selection
n = nobs(female_fit_lasso)
female_lasso_BIC = deviance(female_fit_lasso) + log(n) * (female_fit_lasso$df + 1)
female_coef_lasso = coef(female_fit_lasso)[,which.min(female_lasso_BIC)]
female_lasso_lambda_BIC = female_fit_lasso$lambda[which.min(female_lasso_BIC)]

female_lasso_df = data.frame(norm = abs(coef(female_fit_lasso)[-1,]) %>% apply(2,sum),
                             lambda = female_fit_lasso$lambda,
                             coefficient = c(t(as.matrix(coef(female_fit_lasso)))[,-1]),
                             predictor = rep(rownames(coef(female_fit_lasso))[-1],
                                             each=ncol(coef(female_fit_lasso))))



female_lasso_p = ggplot(female_lasso_df, aes(x=norm, y=coefficient,
                                             col=factor(predictor, labels=c('Age gap','Ambitious',
                                                                            'Attractive', 'Fun',
                                                                            'Intelligent',
                                                                            'Studies', 'Race',
                                                                            'Interests',
                                                                            'Sincere')))) +
  geom_line(size = 0.8) + ggtitle("Female lasso") +
  geom_vline(xintercept = sum(abs(female_coef_lasso[-1])), linetype='dashed') +
  geom_text(x=sum(abs(female_coef_lasso[-1])), y=-0.04, label='BIC selection', colour='black', size=3) +
  labs(x = "L1 norm", y = "Coefficients", col = "Predictor") +
  scale_color_manual(values = colours) +
  theme(panel.background = element_rect(fill = "gray95"))

# cv_female_fit_lasso = cv.glmnet(X,Y, alpha=1, family='binomial')
# bestlamlasso = cv_female_fit_lasso$lambda.min
# female_lasso_coef = predict(female_fit_lasso, type='coefficients', s = bestlamlasso)
# kable(female_lasso_coef[,1], digits=3)
# BIC criterion for the selection of lambda

```
```{r plot lasso, echo=FALSE, fig.height=3.5, fig.width=9, fig.align='center'}
ggarrange(female_lasso_p, male_lasso_p, common.legend = TRUE, legend = "right", ncol=2)

glm_fit_lasso = as.data.frame(rbind(Female = round(female_coef_lasso,2),
                                    Male = round(male_coef_lasso,2)))
colnames(glm_fit_lasso) = c('Intercept', 'Attractive', 'Sincere', 'Intelligent', 'Fun', 'Ambitious', 'Interests', 'Age gap', 'Studies', 'Race')
kable(glm_fit_lasso, label = 'Lasso regression coefficients', align='c', booktabs=T) %>%
  kable_styling(font_size = 7.5) %>%
  kable_styling(latex_options = 'HOLD_position')
```
We can observe a clear division of the predictors: Attractive, Shared Interests and Fun are the most relevant; Race and Age gap do not seem to affect the decision at all; all the remaining predictors have some impact, depending on the gender.
Moreover, we can observe that the order of shrinkage of the coefficients agrees quite well with the ranking given by best subset selection.
```{r male Ridge, eval=F, include=FALSE}
# Lasso seems more useful
X = model.matrix(fit_male_all)[,-1]
Y = data_reg_male_all$dec
male_fit_ridge = glmnet(X,Y, alpha=0, family='binomial')
plot(male_fit_ridge, xlim=c(0,1.6))
text(1.6, coef(male_fit_ridge)[-1,100], #exclude intercept, choose last column, i.e. OLS
      labels = colnames(data_reg_male_all)[-1], cex = 1)

```

#### Comparison bewteen the models

```{r mse, include=F}
mse = function(y, yhat){
  return(mean((y - yhat)^2))
}
```
```{r CV female lasso, include=F}
# Let us compute the MSE of the Lasso model on females data set with a 5-fold CV
K = 5 # number of folds
CV_data = data_reg_female_all
n = nrow(CV_data)
set.seed(42)
# Split the indexes in K folds, the last one will be a little larger
folds = split(sample(n), as.factor(c(rep(1:(K-1), each=n%/%K), rep(K, n%/%K + n%%K))))
MSE = rep(0,K)
for(k in 1:K){
  test = folds[[k]] #indexes of the current test set
  test_data = CV_data[test,]
  test_glm = glm(dec ~., data = test_data, family = 'binomial') # just for the extraction of the model matrix
  X_test = model.matrix(test_glm)[,-1]
  Y_test = test_data$dec
  
  train_data = CV_data[-test,]
  train_glm = glm(dec ~., data = train_data, family = 'binomial') # just for the extraction of the model matrix
  X_train = model.matrix(train_glm)[,-1]
  Y_train = train_data$dec
  
  train_lasso = glmnet(X_train, Y_train, family = 'binomial', lambda = female_lasso_lambda_BIC)
  MSE[k] = predict(train_lasso, newx=X_test, type='response') %>% mse(Y_test)
}
(cv_female_lasso = mean(MSE))
```
```{r CV male lasso, include=F}
# Let us compute the MSE of the Lasso model on males data set with a 5-fold CV
K = 5 # number of folds
CV_data = data_reg_male_all
n = nrow(CV_data)
set.seed(42)
folds = split(sample(n), as.factor(c(rep(1:(K-1), each=n%/%K), rep(K, n%/%K + n%%K))))
MSE = rep(0,K)
for(k in 1:K){
  test = folds[[k]]
  test_data = CV_data[test,]
  test_glm = glm(dec ~., data = test_data, family = 'binomial')
  X_test = model.matrix(test_glm)[,-1]
  Y_test = test_data$dec
  
  train_data = CV_data[-test,]
  train_glm = glm(dec ~., data = train_data, family = 'binomial')
  X_train = model.matrix(train_glm)[,-1]
  Y_train = train_data$dec
  
  train_lasso = glmnet(X_train, Y_train, family = 'binomial', lambda = female_lasso_lambda_BIC)
  MSE[k] = predict(train_lasso, newx=X_test, type='response') %>% mse(Y_test)
}
(cv_male_lasso = mean(MSE))
```
```{r cv comparison female, include=F, fig.align='center'}
# compute MSE of the other models with CV
cv_female_all = cv.glm(data=data_reg_female_all, fit_female_all, K=5, cost=mse)$delta[2]
cv_female_bss = cv.glm(data=data_reg_female_bss, fit_female_bss, K=5, cost=mse)$delta[2]

# Create a data frame to summarize the models
comparison_female = rbind(All = round(fit_female_all$coefficients, 2), 
                          BSS = c(fit_female_bss$coefficients[1:2], 0, 0,
                                  fit_female_bss$coefficients[3:5], rep(0,3)) %>% round(2), #padding zeros
                          Lasso = round(female_coef_lasso,2)) %>%
  cbind(c(cv_female_all, cv_female_bss, cv_female_lasso) %>% round(3),.) %>%
  as.data.frame()
colnames(comparison_female) = c("MSE", 'Intercept', 'Attractive', 'Sincere',
                                'Intelligent', 'Fun', 'Ambitious', 'Interests',
                                "Age gap", "Studies", "Race")
```
```{r cv comparison male, include=F, fig.align='center'}
cv_male_all = cv.glm(data=data_reg_male_all, fit_male_all, K=5, cost=mse)$delta[2]
cv_male_bss = cv.glm(data=data_reg_male_bss, fit_male_bss, K=5, cost=mse)$delta[2]

comparison_male = rbind(All = round(fit_male_all$coefficients, 2), 
                        BSS = c(fit_male_bss$coefficients[1:3], 0,
                                fit_male_bss$coefficients[4:6], rep(0,3)) %>% round(2),
                        Lasso = round(male_coef_lasso,2)) %>%
  cbind(c(cv_male_all, cv_male_bss, cv_male_lasso) %>% round(3),.) %>%
  as.data.frame()
colnames(comparison_male) = c("MSE", 'Intercept', 'Attractive', 'Sincere',
                              'Intelligent', 'Fun', 'Ambitious', 'Interests', 
                              "Age gap", "Studies", "Race")
```
The tables below shows that there is almost no difference between the mean squared errors obtained through 5-fold cross validation: we prefer to use the model chosen by BSS for both genders as it is the most sparse.

```{r cv results show, echo=F, fig.align='center'}
# kable(comparison_female, label = 'CV comparison for females', align='c', booktabs=T) %>%
#   kable_styling(font_size = 7.5)  %>%
#   kable_styling(latex_options = 'HOLD_position') %>%
#   add_header_above(c('FEMALES' = 12))
# 
# kable(comparison_male, label = 'CV comparison for females', align='c', booktabs=T) %>%
#   kable_styling(font_size = 7.5) %>%
#   kable_styling(latex_options = 'HOLD_position') %>%
#   add_header_above(c('MALES' = 12))

comparison = rbind(comparison_female, comparison_male)
row.names(comparison) = c('All', 'BSS', 'Lasso', 'All ', 'BSS ', 'Lasso ')
kable(comparison, label = 'CV comparison for females', align='c', booktabs=T) %>%
  kable_styling(font_size = 7.5) %>%
  kable_styling(latex_options = 'HOLD_position') %>%
  kable_styling(latex_options = "striped", stripe_index = c(1,3,4,6)) %>%
  pack_rows(index = c('FEMALES' = 3, 'MALES' = 3))
```

#### Decision trees
Now, let us support our analysis with some decision trees. They allow to have a better grasp on the classification by visualizing the process which brings to the final decision on the partner. 
```{r female tree, include = FALSE}
female_tree = tree(factor(dec, levels=c(0,1), labels=c('No','Yes')) ~., data=data_reg_female6,
                   control = tree.control(nrow(data_reg_female6), mindev=0.0035))
pruned_female_tree = prune.misclass(female_tree, best=5)
#plot(female_tree, type='uniform')
plot(pruned_female_tree, type='uniform')
title(main = "FEMALE")
#text(female_tree)
text(pruned_female_tree)
```
```{r male tree, include=FALSE, fig.width=9, fig.height=3}
male_tree = tree(factor(dec, levels=c(0,1), labels=c('No','Yes'))~., data=data_reg_male6,
                 control = tree.control(nrow(data_reg_male6), mindev=0.005))
pruned_male_tree = prune.misclass(male_tree, best=4)

par(mfrow=c(1,2))

plot(pruned_female_tree, type='uniform')
title(main = "Female decision tree")
text(pruned_female_tree)

plot(pruned_male_tree, type='uniform')
title(main = "Male decision tree")
text(pruned_male_tree)
```
```{r nice trees, echo=FALSE, fig.width=10, fig.height=2.3}
colnames(data_reg_female6)[2] = "attractive"
colnames(data_reg_female6)[7] = "shared interests"

nice_female_tree <- rpart(factor(dec, levels=c(0,1), labels=c('No','Yes')) ~., data=data_reg_female6, cp = .02)

colnames(data_reg_male6)[2] = "attractive"
colnames(data_reg_male6)[7] = "shared interests"

nice_male_tree <- rpart(factor(dec, levels=c(0,1), labels=c('No','Yes')) ~., data=data_reg_male6, cp = .01)

par(mfrow=c(1,2))

rpart.plot(nice_female_tree, type = 0, under = TRUE, yesno = 0,
           box.palette = c("red2", "springgreen3"), main = "Female decision tree")
# title(main = "Female decision tree")
rpart.plot(nice_male_tree, type = 0, under = TRUE, yesno = 0,
           box.palette = c("red2", "springgreen3"), main = "Male decision tree")
# title(main = "Male decision tree")
```
<!-- 0.** : probability of yes, % : percentage of obs in each rule -->
Once again, physical appealing is confirmed to be the most relevant attribute both genders look at when choosing a possible partner: if a person is not considered attractive enough, he/she is rejected. Shared interests are the second thing taken into consideration: it is indeed an indicator of compatibility, however the threshold required by females is far higher. If this criterion is not met, males look for a fun partner, whereas females require the date to be really attractive.
Below we can find the partition plots of the trees: they partition the feature space into a number of smaller and non-overlapping regions with similar response values according to the splitting rules. As the male decision tree took into account three attributes, there are two plots: in the second one we are assumig that the score for attractiveness is higher than 6.25.
```{r female partition plot, include=F}
# Hand-made partition plot with jittered predictors
female_partition = ggplot(data_reg_female_all, aes(x=attr, y=shar)) +
  geom_jitter(aes(col=factor(dec, levels=c(0,1), labels=c('No','Yes'))), 
              alpha=0.7, width = 0.18, height = 0.2, size = 1.2) +
  scale_color_manual(values=c("red2", "springgreen3")) +
  ggtitle("Female partition plot") +
  scale_x_continuous(expand=c(.05,.05)) +
  scale_y_continuous(expand=c(.05,.05)) +
  ylab("Shared interests") + xlab("Attractive") + labs(col = 'Decision') +
  geom_vline(xintercept = 6.75) +
  geom_segment(y = 6.5, yend = 6.5, x = 6.75, xend = 11) +
  geom_segment(y = 3.5, yend = 3.5, x = 6.75, xend = 11) +
  geom_segment(y = 3.5, yend = 6.5, x = 7.25, xend = 7.25) +
  geom_text(x = 3, y = 5, label = "NO") +
  geom_text(x = 8.75, y = 8.75, label = "YES") +
  geom_text(x = 7, y = 5, label = "NO") +
  geom_text(x = 8.75, y = 5, label = "YES") +
  geom_text(x = 8.75, y = 1.25, label = "NO")
```
```{r male partition plot, include=F}
male_partition = ggplot(data_reg_male_all, aes(x=attr, y=shar)) +
  geom_jitter(aes(col=factor(dec, levels=c(0,1), labels=c('No','Yes'))), alpha=0.7, width = 0.18, height = 0.2, size = 1.2) +
  scale_color_manual(values=c("red2", "springgreen3")) +
  ggtitle("Male partition plot") +
  scale_x_continuous(expand=c(.05,.05)) +
  scale_y_continuous(expand=c(.05,.05)) +
  ylab("Shared interests") + xlab("Attractive") + labs(col = 'Decision') +
  geom_vline(xintercept = 6.25) +
  geom_segment(y = 4.5, yend = 4.5, x = 6.25, xend = 11) +
  geom_text(x = 3, y = 4.5, label = "NO") +
  geom_text(x = 8.5, y = 1.75, label = "NO") +
  geom_text(x = 8.5, y = 7.55, label = "YES")
```
```{r male partition plot once attractive > 6.25, include = F}
male_partition_2 = ggplot(data = subset(data_reg_male_all, attr>6.25), aes(x=fun, y=shar)) +
  geom_jitter(aes(col=factor(dec, levels=c(0,1), labels=c('No','Yes'))),
              alpha=0.7, width = 0.18, height = 0.2, size = 1.2) +
  scale_color_manual(values=c("red2", "springgreen3")) +
  ggtitle("Male partition plot for attractive > 6.25") +
  scale_x_continuous(expand=c(.05,.05)) +
  scale_y_continuous(expand=c(.05,.05)) +
  ylab("Shared interests") + xlab("Fun") + labs(col = 'Decision') +
  geom_hline(yintercept = 4.5) +
  geom_segment(y = -1, yend = 4.5, x = 6.75, xend = 6.75) +
  geom_text(x = 5, y = 7.5, label = "YES") +
  geom_text(x = 3.1, y = 1.75, label = "NO") +
  geom_text(x = 8.75, y = 1.75, label = "YES")
```
```{r partition plots, echo=FALSE, fig.width=9, fig.height=2.75, fig.align='center'}
ggarrange(female_partition, male_partition, male_partition_2, ncol=3, nrow=1,
          common.legend = TRUE, legend = "right")# %>%
  #annotate_figure(top = text_grob("Partition plots of the trees", face = "bold", size = 13))
```

## What did we learn?
```{r coefficient interpretation, include=F}
# save the rates of odds increase/decrease
worst_odds_male = round(exp(fit_male_bss$coefficients[1]),2)
attr_odds_male = round(exp(fit_male_bss$coefficients[2]),2)
sinc_odds_male = round(exp(-fit_male_bss$coefficients[3]),2)
fun_odds_male = round(exp(fit_male_bss$coefficients[4]),2)
amb_odds_male = round(exp(-fit_male_bss$coefficients[5]),2)
shar_odds_male = round(exp(fit_male_bss$coefficients[6]),2)

worst_odds_female = round(exp(fit_female_bss$coefficients[1]),2)
attr_odds_female = round(exp(fit_female_bss$coefficients[2]),2)
fun_odds_female = round(exp(fit_female_bss$coefficients[3]),2)
amb_odds_female = round(exp(-fit_female_bss$coefficients[4]),2)
shar_odds_female = round(exp(fit_female_bss$coefficients[5]),2)
```
Let us recall the chosen regression model on the decision after the speed date:
```{r BSS model, echo=F}
kable(glm_fit_bss, digits=3, label = 'Gender differences in attributes weights', align='c', booktabs=T) %>%
  kable_styling(font_size = 7.5) %>%
  kable_styling(latex_options = 'HOLD_position')
```
It is all fun and games, but how can we use it?
The model works in the following way: if all 0 scores are assigned to the partner, the odds of a positive decision are around `r toString(worst_odds_female)` for both genders (duh!).
Then, for each attraction point assigned by a woman, the odds of wanting to see the partner again become `r toString(attr_odds_female)` times larger. Analogously, one point assigned to fun or shared interests corresponds to an increase rate for the odds of `r toString(fun_odds_female)` and `r toString(shar_odds_female)` respectively.
On the other hand, one point of ambition makes the odds `r toString(amb_odds_female)` times smaller.
From the point of view of men instead, each attraction point makes the odds `r toString(attr_odds_male)` times higher. Similarly, one point of fun or shared interests corresponds to an increase rate of `r toString(fun_odds_male)` and `r toString(shar_odds_male)`, while one point of sincere or ambition makes the odds `r toString(sinc_odds_male)` and `r toString(amb_odds_male)` times smaller respectively.\newline
The take home is that what counts the most for university students is the first impression you give, as the time is limited. It is hard indeed to appreciate the intelligence of a stranger in 4 minutes. Hence, we rely mostly on our gut feelings, which are heavily influenced by how much we find that person attractive or compatible. If there is match, you will have plenty of time to see whether they checks all the boxes in the long list of attributes that describe our perfect mate. What are you waiting for? Get all dressed up and sign for the next Speed Dating event in your campus!

<!-- Further analyses we have performed -->
```{r accuracy for classification task, include=F}
# accuracy of our model as a classifier, computed through 5-fold cross validation
# is satisfying: about 75% for both genders
acc = function(actual, predicted){
  return(mean(actual == predicted))
}
predclass = function(y){
  classes = rep(NULL, length(y))
  for(i in 1:length(y)){
    if(y[i]<.5){
      classes[i] = 0
    }
    else{
      classes[i] = 1
    }
  }
  return(classes)
}

# males
K = 5 # number of folds
CV_data = data_reg_male_bss
n = nrow(CV_data)
set.seed(42)
folds = split(sample(n), as.factor(c(rep(1:(K-1), each=n%/%K), rep(K, n%/%K + n%%K))))
male_accuracy = rep(0,K)
for(k in 1:K){
  test = folds[[k]]
  test_data = CV_data[test,]
  X_test = select(test_data, -dec)
  Y_test = test_data$dec
  
  train_data = CV_data[-test,]
  train_glm = glm(dec ~., data = train_data, family = 'binomial')
  
  male_accuracy[k] = predict(train_glm, newdata = X_test, type='response') %>% predclass() %>% acc(Y_test)
}
(mean_male_acc = mean(male_accuracy))

# females
K = 5 # number of folds
CV_data = data_reg_female_bss
n = nrow(CV_data)
set.seed(42)
folds = split(sample(n), as.factor(c(rep(1:(K-1), each=n%/%K), rep(K, n%/%K + n%%K))))
female_accuracy = rep(0,K)
for(k in 1:K){
  test = folds[[k]]
  test_data = CV_data[test,]
  X_test = select(test_data, -dec)
  Y_test = test_data$dec
  
  train_data = CV_data[-test,]
  train_glm = glm(dec ~., data = train_data, family = 'binomial')
  
  female_accuracy[k] = predict(train_glm, newdata = X_test, type='response') %>% predclass() %>% acc(Y_test)
}
(mean_female_acc = mean(female_accuracy))
```

```{r binned male, include=F}
par(mfrow=c(2,4))
# Not too bad binned plot
fit_male_bss_r = glm(dec ~ attr + fun + shar + amb + sinc, data=data_reg_male_all, family='binomial')
for(i in (2:(ncol(data_reg_male_all)))[-c(8,9)]){
  binnedplot(x=data_reg_male_all[,i], y=resid(fit_male_bss_r, type="response"),
             cex=.5, main=sprintf(colnames(data_reg_male_all)[i]), ylab='')
}
```
```{r binned female, include=F}
# Not too bad binned plot
par(mfrow=c(2,4))
fit_female_bss_r = glm(dec ~ attr + fun + shar + amb, data=data_reg_female_all, family='binomial')
for(i in (2:(ncol(data_reg_male_all)))[-c(8,9)]){
  binnedplot(x=data_reg_female_all[,i], y=resid(fit_female_bss_r, type="response"),
             cex=.5, main=sprintf(colnames(data_reg_female_all)[i]), ylab='')
}
```